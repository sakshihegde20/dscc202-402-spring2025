{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a504857a-b283-47d5-ab8f-0928e2b9b508",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Best Practices for DataFrame Caching**\n",
    "\n",
    "**Key Takeaways: Smart Caching = Efficient Performance**\n",
    "\n",
    "### 1. Cache Selectively: Only When It's Worth It\n",
    "*   **Don't cache by default:** Avoid caching DataFrames unless they will be used multiple times.\n",
    "*   **Focus on frequently reused data:** Prioritize caching DataFrames used in:\n",
    "    *   Exploratory Data Analysis (EDA)\n",
    "    *   Machine Learning Training Datasets\n",
    "    *   Intermediate results that are needed by multiple downstream transformations\n",
    "*   **Think before caching:** Evaluate if the caching overhead will be worth the benefit.\n",
    "\n",
    "### 2. Reduce Storage Footprint\n",
    "*   **Omit unnecessary columns:** Remove any columns not needed for downstream tasks before caching.\n",
    "*   **Minimize data size:** Smaller DataFrames lead to faster loading and more efficient memory/disk usage.\n",
    "*   **Use correct data types:** Ensure that the data has the right data type to minimize overhead (eg. reduce an integer to short)\n",
    "\n",
    "### 3. Materialize with a Fast Action\n",
    "*   **`.cache()` is lazy:** It merely marks the DataFrame for caching.\n",
    "*   **Use a fast action (e.g., `count()`):** Triggers materialization of the cached data by forcing an actual compute.\n",
    "*   **Data that goes into the action is what's cached**: not the output of the action.\n",
    "*   **Avoid materializing with slow actions**: Avoid using slow actions like writing to files to force the materialization since that can be an unnecessary compute.\n",
    "\n",
    "### 4. Evict When No Longer Needed\n",
    "*   **Manage cache proactively:** Remove DataFrames from the cache when they are no longer in use to free up resources.\n",
    "*   **Avoid memory issues:** Timely eviction of cached data prevents memory exhaustion.\n",
    "*   **Use `unpersist()`:** The function in Spark to remove cache.\n",
    "\n",
    "\n",
    "**Simplified Caching Process:**\n",
    "\n",
    "```\n",
    "Original Data -> Transformations (select, filter, etc.) -> .cache() -> Fast Action (count) -> [Cached Data]\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf8a04e-06f2-49df-a277-2b38c5804962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. Generate a Test DataFrame\n",
    "data = [(\"Alice\", 30, \"Engineer\"),\n",
    "        (\"Bob\", 25, \"Data Scientist\"),\n",
    "        (\"Charlie\", 35, \"Manager\"),\n",
    "        (\"Alice\", 32, \"Data Analyst\"),\n",
    "        (\"Bob\", 28, \"Engineer\"),\n",
    "        (\"Dave\", 40, \"Manager\"),\n",
    "        (\"Eve\", 26, \"Data Scientist\"),\n",
    "        (\"Frank\", 31, \"Engineer\")]\n",
    "\n",
    "schema = [\"name\", \"age\", \"job_title\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "print(\"Initial DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# 2. Define Transformations\n",
    "\n",
    "# Select only the name and job_title columns\n",
    "selected_df = df.select(\"name\", \"job_title\")\n",
    "\n",
    "# Filter to remove people who are managers\n",
    "filtered_df = selected_df.filter(col(\"job_title\") != \"Manager\")\n",
    "\n",
    "# Caching the filtered data before the group by\n",
    "print(\"Caching the filtered data...\")\n",
    "filtered_df.cache()  # Marks the filtered data for caching, no computation yet.\n",
    "\n",
    "\n",
    "# 3. Materialize the Cache with a Fast Action\n",
    "print(\"Materializing Cache with count()...\")\n",
    "filtered_df.count()  # This will execute all transformations and the cache will be populated.\n",
    "\n",
    "# 4. Subsequent Usage (Reading from Cache - accessing original data)\n",
    "print(\"\\nSubsequent Usage (reading from cache, before count):\")\n",
    "filtered_df.show()  # Reads the cached filtered data\n",
    "\n",
    "# 5. Another Subsequent Usage with a filter. (after count)\n",
    "filtered_grouped_df = filtered_df.filter(col(\"job_title\") == 'Engineer') # Reads from the cache then applies the filter\n",
    "print(\"\\nSubsequent Usage (reading from cache after count with a filter):\")\n",
    "filtered_grouped_df.show()\n",
    "\n",
    "# 6. Another Subsequent Usage with a different column selected. (after count)\n",
    "filtered_grouped_df_2 = filtered_df.select(col(\"job_title\")) # Reads from the cache then does the select\n",
    "print(\"\\nSubsequent Usage (reading from cache after count with a select):\")\n",
    "filtered_grouped_df_2.show()\n",
    "\n",
    "# 7. Unpersist the DataFrame\n",
    "print(\"Unpersisting the cache...\")\n",
    "filtered_df.unpersist()\n",
    "\n",
    "# Try to force the computation\n",
    "print(\"\\nTrying to read the dataframe after unpersisting...\")\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea89634c-bf8f-4757-a508-51e243ba01c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## cache() vs. persist(): What is the difference\n",
    "**`cache()`: A Convenience Shortcut**\n",
    "\n",
    "*   **Equivalent to `persist(StorageLevel.MEMORY_ONLY)`:**  The `cache()` function is essentially a convenient shortcut. When you call `df.cache()`, Spark is actually doing the equivalent of `df.persist(StorageLevel.MEMORY_ONLY)`.\n",
    "*   **Intended for Memory Storage:** The `MEMORY_ONLY` storage level implies that Spark *tries* to store the data in memory first.\n",
    "*   **Not a Guarantee of Memory Storage:** It's important to understand that `MEMORY_ONLY` does **not** guarantee that the data will *always* be stored in memory. If there isn't enough memory available, Spark may still have to evict some data to make room, potentially writing it to disk or completely recomputing it in future accesses.\n",
    "*   **Default Behavior:** It sets a default storage preference for the data.\n",
    "\n",
    "**`persist()`: Explicit Control Over Storage**\n",
    "\n",
    "*   **Allows Specifying Storage Level:** The `persist()` function allows you to explicitly specify the storage level you want to use for your data. You can choose between:\n",
    "    *   `MEMORY_ONLY`: Store in memory if possible; if there is not enough memory then data can be recomputed.\n",
    "    *   `MEMORY_ONLY_SER`: Store in memory as serialized data.\n",
    "    *   `MEMORY_AND_DISK`: Store in memory if possible; otherwise, store to disk.\n",
    "    *   `MEMORY_AND_DISK_SER`: Store in memory as serialized data if possible; otherwise, store to disk.\n",
    "    *   `DISK_ONLY`: Store only on disk.\n",
    "    *   `MEMORY_ONLY_2`, `MEMORY_AND_DISK_2`: Store in memory with replication factor of 2 (for fault tolerance).\n",
    "    *   And more, see Storage Levels in Spark's documentation.\n",
    "*   **More Granular Control:** `persist()` gives you finer control over how and where your cached data is stored. You can tailor the storage strategy to the specific needs of your application and the resources available.\n",
    "*   **More Flexibility**: You can control replication and if the data is serialized before being cached.\n",
    "\n",
    "**Key Differences Summarized**\n",
    "\n",
    "| Feature           | `cache()`                             | `persist()`                                                                            |\n",
    "| :---------------- | :------------------------------------ | :------------------------------------------------------------------------------------- |\n",
    "| Storage Level     | `MEMORY_ONLY` (implicit)               | Explicitly configurable (e.g., `MEMORY_ONLY`, `MEMORY_AND_DISK`, `DISK_ONLY`, etc.) |\n",
    "| Flexibility       | Limited to memory-first strategy     | Offers fine-grained control over storage location, persistence options.              |\n",
    "| Functionality  | Shortcut for `persist(StorageLevel.MEMORY_ONLY)` | Used to choose a specific storage level    |\n",
    "\n",
    "**Important Considerations:**\n",
    "\n",
    "*   **Memory Limits:** Spark operates within the confines of the available cluster memory. Regardless of the storage level you choose, Spark might need to evict cached data if memory pressure is too high.\n",
    "*   **Disk Spill:** Even when `MEMORY_ONLY` or `MEMORY_ONLY_SER` is chosen, the data could still spill to disk if memory is insufficient.\n",
    "*   **Serialization:** When data is stored in memory, it's always serialized at some level of abstraction for efficient storage and retrieval within the JVM. This is done implicitly with `MEMORY_ONLY` and explicitly using the `_SER` options.\n",
    "*   **Tradeoffs:** When deciding to persist data to disk, consider the overhead it takes to serialize and write the data to disk as well as read the data from disk in the subsequent calls.\n",
    "\n",
    "**Which to Use?**\n",
    "\n",
    "*   **`cache()`:** Is a great choice when you're mostly concerned about in-memory performance, want a simple solution, and are comfortable with Spark's default memory management behavior. For most cases, this is usually fine.\n",
    "*   **`persist()`:** Is preferred when you have particular needs to optimize storage:\n",
    "    *   When you're working with datasets that may not fit into memory and you are willing to use disk\n",
    "    *   When you need to use replication for data fault-tolerance\n",
    "    *   When you need to choose disk only for very large datasets\n",
    "    *   When you need to serialize the data in memory for memory savings\n",
    "\n",
    "**In Summary**\n",
    "\n",
    "*   `cache()` is a shortcut for `persist(StorageLevel.MEMORY_ONLY)` and prioritizes in-memory storage, but does not guarantee in-memory caching.\n",
    "*   `persist()` offers more control by allowing you to explicitly specify a storage level, including options for memory, disk, and serialization.\n",
    "*   Both methods can spill data to disk if not enough memory is available. The difference is that with `persist()` you can ask to spill to disk and specify the manner.\n",
    "\n",
    "Understanding this distinction is crucial for effective resource management and performance tuning when working with Spark, since it impacts where your data is cached (memory vs disk).\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark Caching Best Practices",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
