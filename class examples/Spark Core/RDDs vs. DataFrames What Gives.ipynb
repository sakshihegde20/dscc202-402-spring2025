{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb18f8c0-e1ba-42c0-a645-73549050e8d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\"In Apache Spark, **_RDDs (Resilient Distributed Datasets)_** are the fundamental, low-level data structure representing collections of elements. DataFrames, on the other hand, are a higher-level abstraction built on top of RDDs, providing a structured view of data with named columns and a schema. While RDDs offer flexibility, DataFrames provide more optimization opportunities through Spark's Catalyst optimizer, which can significantly improve query performance. DataFrames are now the primary API for most Spark users due to their ease of use and optimization capabilities.\"\n",
    "\n",
    "<img src=\"https://data-science-at-scale.s3.us-east-1.amazonaws.com/images/rdd_dataframe_lineage.png\" width=\"640\">\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1.  **Foundation (RDDs):**\n",
    "    *   Think of RDDs as the \"assembly language\" of Spark. They are the underlying building blocks of all Spark computations.\n",
    "    *   They represent a collection of elements that are partitioned across a cluster of machines, allowing for parallel processing.\n",
    "    *   RDDs are inherently flexible – they can hold any type of data (e.g., raw text, objects, etc.).\n",
    "    *   They expose low-level transformations and actions, giving developers fine-grained control over processing.\n",
    "    *   However, this low-level control also means developers need to be more aware of optimization details.\n",
    "\n",
    "2.  **Structured View (DataFrames):**\n",
    "    *   DataFrames are like a \"spreadsheet\" or a \"table\" with columns and rows, where each column has a name and a specific data type (a schema).\n",
    "    *   They are built on top of RDDs, and internally, they are a set of RDDs which hold Row objects.\n",
    "    *   They bring structure to data, which allows Spark to reason about data and make optimization decisions.\n",
    "    *   DataFrames utilize Spark's Catalyst optimizer, which analyzes execution plans and applies techniques like predicate pushdown, column pruning, and optimized join strategies, leading to increased performance.\n",
    "    *   DataFrames are typically easier to use for most common data processing operations, as you can use SQL-like syntax for querying and manipulating data.\n",
    "    *   DataFrames are the preferred API for new Spark applications, given their ease of use and improved performance over RDDs.\n",
    "\n",
    "3.  **Key Differences (Table):**\n",
    "\n",
    "| Feature         | RDD                                  | DataFrame                            |\n",
    "|-----------------|--------------------------------------|----------------------------------------|\n",
    "| **Level**       | Low-level                             | High-level                              |\n",
    "| **Structure**   | Unstructured or semi-structured      | Structured with named columns & schema|\n",
    "| **Optimization**| Requires manual optimization          | Optimized by Catalyst optimizer      |\n",
    "| **Ease of Use**  | More flexible, less intuitive        | Easier, more intuitive                |\n",
    "| **Performance**| Potentially slower, requires optimization | Generally faster due to optimization |\n",
    "| **Data Types**   | Can handle any data type           | Works with structured and typed data    |\n",
    "\n",
    "4.  **Analogy:**\n",
    "\n",
    "*   Imagine RDDs as a box of random, unsorted objects. You can access each object individually, but you have to manually organize them to extract meaningful information.\n",
    "*   DataFrames are like a neatly organized filing cabinet, where each folder is a column and documents within are the rows. You can quickly search and filter based on the folders (column names) and extract information effectively.\n",
    "\n",
    "5.  **Interoperability:**\n",
    "\n",
    "*   You can convert RDDs to DataFrames when needed using the `createDataFrame` method, and vice versa. You can also use RDD methods on a DataFrame by first converting it to a RDD object.\n",
    "*   This ability to convert between RDDs and DataFrames allows for a gradual transition or mix-and-match based on specific use cases.\n",
    "*   When custom operations that are not readily available on DataFrames are required, a developer may choose to manipulate RDDs directly.\n",
    "\n",
    "**When to use RDDs?**\n",
    "\n",
    "*   When you need very fine-grained control over data processing.\n",
    "*   When working with unstructured data or custom data types that DataFrames can’t easily handle\n",
    "* When you need to do very custom manipulations that are not readily available in the DataFrame API.\n",
    "\n",
    "**Key Takeaway:**\n",
    "\n",
    "**_DataFrames build on top of RDDs, offering a structured and optimized way to process data with less coding needed for optimal performance, making them the preferred API for most use cases. RDDs are best used for specialized use cases that require more fine-grained control._**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8541c30e-e493-49e9-985f-3eed1b820365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## RDD Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4adf364-a056-4eb9-9170-671ec8dd041c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Read text file into an RDD\n",
    "lines_rdd = spark.read.text(\"dbfs:/FileStore/sherlock.txt\").rdd.map(lambda row: row[0])\n",
    "\n",
    "# 2. Split into words and flatten, filter and make key-value pairs\n",
    "word_counts_rdd = (\n",
    "    lines_rdd.flatMap(lambda line: line.split(\" \"))\n",
    "        .filter(lambda word: len(word) > 0)\n",
    "        .map(lambda word: (word, 1))\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "\n",
    "# 3. Sort and take top 10\n",
    "top_10_rdd = word_counts_rdd.sortBy(lambda x: x[1], ascending=False).take(10)\n",
    "\n",
    "# 4. Print the top 10 words\n",
    "print(\"Top 10 words using RDD:\")\n",
    "for word, count in top_10_rdd:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29d13a1b-5c9c-48ea-9050-a72bcef71a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DataFrame Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f1574df-a317-413a-9ff7-6831d023076a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import split, explode, col, length\n",
    "\n",
    "# 1. Read text file into a DataFrame\n",
    "lines_df = spark.read.text(\"dbfs:/FileStore/sherlock.txt\")\n",
    "\n",
    "# 2. Split into words and explode and filter\n",
    "words_df = lines_df.withColumn(\"word\", explode(split(col(\"value\"), \" \"))).filter(length(col(\"word\")) >= 1)\n",
    "\n",
    "# 3. Group, count, sort and limit\n",
    "word_counts_df = (\n",
    "    words_df.groupBy(\"word\")\n",
    "        .count()\n",
    "        .orderBy(col(\"count\"), ascending=False)\n",
    "        .limit(10)\n",
    ")\n",
    "\n",
    "# 4. Show top 10 words\n",
    "print(\"Top 10 words using DataFrame:\")\n",
    "word_counts_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6866d4a1-fc11-4f38-8bb1-7d65d0edd1eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Observations:**\n",
    "\n",
    "*   **Level of Abstraction:**\n",
    "    *   **RDD:** The RDD code is more verbose and requires you to explicitly define every step of the computation. You manipulate individual elements (lines and words) directly through `map`, `flatMap`, etc. This is a low-level style that offers more flexibility, but also requires more effort.\n",
    "    *   **DataFrame:** The DataFrame code is more concise and uses higher-level operations like `withColumn`, `explode`, `groupBy`, and `orderBy`. These are declarative operations that express what you want to do, not how, abstracting away the low-level details.\n",
    "*   **Data Structure:**\n",
    "    *   **RDD:** The RDD code uses raw text strings and tuples to represent the data, requiring developers to manually manage data structure and perform aggregations. The RDD is a collection of generic objects.\n",
    "    *   **DataFrame:** The DataFrame code uses structured DataFrames with named columns (\"value,\" \"word,\" \"count\").  The structure allows Spark to understand what is meant by the code.\n",
    "*   **Optimization:**\n",
    "    *   **RDD:**  Requires the developer to manage the optimization process. `reduceByKey` can perform well in this case, however, more complex aggregations or data manipulation may require more thinking on how to optimize.\n",
    "    *   **DataFrame:** The DataFrame code automatically leverages Spark's Catalyst optimizer. Even though the code is very explicit with operations like `groupBy` and `orderBy`, the optimizer is free to choose an optimal execution plan. For example, if you have more columns in a DataFrame, it can optimize away operations to select just a few columns.\n",
    "*   **Ease of Use:**\n",
    "    *   **RDD:** The RDD code might be less intuitive for some users, especially those not familiar with functional programming style.\n",
    "    *   **DataFrame:** The DataFrame code is more readable and easier to understand due to its use of high level functions and schema.\n",
    "* **Interoperability**\n",
    "    * Note that both of these approaches read text files using a sparkSession. The first approach extracts an RDD from a DataFrame read, while the second approach operates directly on a DataFrame after the data is read.\n",
    "    * You can always extract a RDD from a dataframe as we did in the first example, and then operate on it using RDD operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9496a2da-9b9f-49f8-b8b8-746036b2c876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Example 2: Conversion Between RDD and DataFrame**\n",
    "\n",
    "This example shows how to convert an RDD to a DataFrame and vice versa, highlighting their interoperability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "734dbdd8-26a2-4377-a7f2-0450db487657",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# 1. Create an RDD of key value pairs (as a simple example)\n",
    "data_rdd = spark.sparkContext.parallelize([(\"apple\", 5), (\"banana\", 10), (\"apple\", 2)])\n",
    "\n",
    "# 2. Convert the RDD to a DataFrame with column names\n",
    "schema = StructType([\n",
    "    StructField(\"fruit\", StringType(), True),\n",
    "    StructField(\"count\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_from_rdd = spark.createDataFrame(data_rdd, schema)\n",
    "\n",
    "# 3. Print the dataFrame\n",
    "print(\"DataFrame from RDD:\")\n",
    "df_from_rdd.show()\n",
    "\n",
    "# 4. Convert a DataFrame to RDD (for example purposes only)\n",
    "rdd_from_df = df_from_rdd.rdd.map(lambda row: (row[0], row[1]))\n",
    "\n",
    "# 5. Print elements from RDD extracted from DataFrame\n",
    "print(\"RDD from DataFrame:\")\n",
    "for element in rdd_from_df.collect():\n",
    "  print(element)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15f7ac97-424e-493a-9ded-3e9329cc8dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Observations:**\n",
    "\n",
    "*   **Flexibility of RDDs:** The `data_rdd` here shows that an RDD can be created in any way, even just using `parallelize`. The RDDs can have custom datatypes and custom data structures.\n",
    "*   **`createDataFrame` and Schema**: To create a DataFrame from an RDD, we have to explicitly provide a schema that defines the column names and their data types. DataFrames are structured and require a schema.\n",
    "*   **`rdd` property:**  The `.rdd` property of a DataFrame can be used to access the underlying RDD, and you can manipulate it using the RDD API, when needed.\n",
    "*   **Interoperability**: This shows that you can easily switch between RDDs and DataFrames, based on the needs of your workflow.\n",
    "\n",
    "**In Conclusion:**\n",
    "\n",
    "These examples highlight the main differences and relationships between RDDs and DataFrames in Apache Spark. The RDD API offers a lower-level approach to data manipulation, while the DataFrame API provides a higher-level abstraction with ease of use and performance optimization advantages. The choice between RDDs and DataFrames depends on your specific use case, requirements, and expertise. However, for most cases, DataFrames are the preferred option because of their advantages of performance and conciseness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cccd09c-1fa9-442f-a3f1-0b4ef190e9d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "RDDs vs. DataFrames What Gives",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
