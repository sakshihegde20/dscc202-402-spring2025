{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a212a052-af98-4ef6-aa58-bfcf8d76786e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##SparkSession Methods\n",
    "1.  **`sql()`**:\n",
    "    *   This method lets you execute SQL queries against your Spark data.\n",
    "    *   It returns a DataFrame containing the results.\n",
    "    *   You need to register tables or views before querying. (you can do it by reading a table into a DataFrame with `spark.read` and then calling `df.createOrReplaceTempView(\"my_table\")`.)\n",
    "    *   This is useful for complex transformations or using SQL's strengths.\n",
    "\n",
    "2.  **`table()`**:\n",
    "    *   This method retrieves a DataFrame that represents an existing table in the metastore.\n",
    "    *   You need to have a table that's been created (either by `CREATE TABLE` SQL, Spark jobs etc.)\n",
    "    *   This is a quick way to load tables for further processing.\n",
    "\n",
    "3.  **`read()`**:\n",
    "    *   This returns a `DataFrameReader` object, which is used to read different types of data.\n",
    "    *   You use methods on the `DataFrameReader` (like `.csv()`, `.parquet()`, `.json()`, etc.) to specify the data source and any options (header, schema).\n",
    "    *   This is the entry point for loading data from external files or databases.\n",
    "\n",
    "4. **`range()`**:\n",
    "   *  This method creates a DataFrame with a column of numbers following a specific range and step.\n",
    "   *   It is useful for creating a test dataset and for adding a number index to the DataFrame.\n",
    "   *   The arguments are `start`, `end` (exclusive), `step` (optional, defaults to 1), and number of `partitions` (optional, defaults to 1)\n",
    "\n",
    "**Important Notes:**\n",
    "\n",
    "*   **SparkSession:**  You must create a `SparkSession` to interact with Spark. This is created for you in Databricks.\n",
    "*   **Paths:**  Make sure to adjust file paths to where your data is located.\n",
    "*   **Schema Inference:**  `inferSchema=True` in `read.csv()` attempts to infer the data types of columns. Use carefully, it can be slow with big files, is better to provide a schema.\n",
    "*   **Show:** The `.show()` method displays a few rows of the DataFrame for visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da1b8af4-880d-4ea9-94fb-174cf7a4562a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import os\n",
    "\n",
    "# 1. Create a DataFrame with 100 rows\n",
    "# ----------------------------------\n",
    "num_rows = 100\n",
    "\n",
    "# Create a schema with two columns\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"value\", StringType(), True),\n",
    "])\n",
    "# Create the data as a list\n",
    "data = [(i, f\"row_{i}\") for i in range(num_rows)]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Register the DataFrames as Temporary View\n",
    "# ----------------------------------------------------\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "# 2. Define output paths\n",
    "# -----------------------\n",
    "output_path_csv = \"/tmp/my_data.csv\"\n",
    "output_path_json = \"/tmp/my_data.json\"\n",
    "output_path_parquet = \"/tmp/my_data.parquet\"\n",
    "\n",
    "# 3. Save the DataFrame to CSV, JSON, and Parquet\n",
    "# ---------------------------------------------\n",
    "df.write.csv(output_path_csv, header=True, mode=\"overwrite\")\n",
    "df.write.json(output_path_json, mode=\"overwrite\")\n",
    "df.write.parquet(output_path_parquet, mode=\"overwrite\")\n",
    "\n",
    "# 4. Load the data back into DataFrames\n",
    "# ------------------------------------\n",
    "df_csv_loaded = spark.read.csv(output_path_csv, header=True, inferSchema=True)\n",
    "df_json_loaded = spark.read.json(output_path_json)\n",
    "df_parquet_loaded = spark.read.parquet(output_path_parquet)\n",
    "\n",
    "# Example: Run a SQL query and get the results as a DataFrame\n",
    "query_result_df = spark.sql(\"SELECT * FROM my_table WHERE id > 97\")\n",
    "query_result_df.show() # Show sample rows from the DataFrame\n",
    "\n",
    "# Example: Load data from a table\n",
    "table_df = spark.table(\"my_table\")  # The table needs to exist in the current database\n",
    "table_df.show()\n",
    "\n",
    "# Example: Create a DataFrame with numbers from 0 to 9\n",
    "range_df_1 = spark.range(0, 10)  # defaults to step 1 and 1 partition\n",
    "range_df_1.show()\n",
    "\n",
    "# Example:  Create a DataFrame with numbers from 10 to 100, step of 2 and 2 partitions.\n",
    "range_df_2 = spark.range(10, 100, 2, 2) # start, end, step, number of partitions\n",
    "range_df_2.show()\n",
    "\n",
    "# Example: Create a DataFrame with a range and custom column name\n",
    "range_df_3 = spark.range(10, 20).withColumnRenamed(\"id\",\"my_custom_range_column\")\n",
    "range_df_3.show()\n",
    "\n",
    "# Clean up the output path (if needed)\n",
    "# ------------------------------------\n",
    "def delete_files(path):\n",
    "    if os.path.exists(path):\n",
    "        if os.path.isdir(path):\n",
    "            for file in os.listdir(path):\n",
    "                os.remove(os.path.join(path, file))\n",
    "            os.rmdir(path)\n",
    "        else:\n",
    "            os.remove(path)\n",
    "\n",
    "delete_files(output_path_csv)\n",
    "delete_files(output_path_json)\n",
    "delete_files(output_path_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc26fb6-8f2b-420e-bca7-0a33ad071133",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark Function and Method Summary\n",
    "\n",
    "| Category                 | Method/Function               | Description                                                                                                                                                            |\n",
    "|--------------------------|-------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **SparkSession Methods** | `sql`                          | Returns a DataFrame representing the result of the given query.                                                                                                       |\n",
    "|                          | `table`                        | Returns the specified table as a DataFrame.                                                                                                                            |\n",
    "|                          | `read`                         | Returns a DataFrameReader that can be used to read data in as a DataFrame.                                                                                           |\n",
    "|                          | `range`                        | Creates a DataFrame with a column containing elements in a range from start to end (exclusive) with a step value and number of partitions.                              |\n",
    "| **DataFrame Transformation Methods**| `select`                    | Returns a new DataFrame by computing given expressions for each element.                                                                                         |\n",
    "|                          | `drop`                        | Returns a new DataFrame with a column dropped.                                                                                                                         |\n",
    "|                          | `withColumnRenamed`           | Returns a new DataFrame with a column renamed.                                                                                                                         |\n",
    "|                          | `withColumn`                  | Returns a new DataFrame by adding a column or replacing the existing column that has the same name.                                                                 |\n",
    "|                          | `filter`, `where`             | Filters rows using the given condition.                                                                                                                                  |\n",
    "|                          | `sort`, `orderBy`             | Returns a new DataFrame sorted by the given expressions.                                                                                                                   |\n",
    "|                          | `dropDuplicates`, `distinct` | Returns a new DataFrame with duplicate rows removed.                                                                                                                |\n",
    "|                          | `limit`                       | Returns a new DataFrame by taking the first n rows.                                                                                                                       |\n",
    "|                          | `groupBy`                     | Groups the DataFrame using the specified columns, so aggregations can be run on them.                                                                                     |\n",
    "| **DataFrame Action Methods**| `show`                        | Displays the top n rows of a DataFrame in a tabular form.                                                                                                               |\n",
    "|                          | `count`                       | Returns the number of rows in the DataFrame.                                                                                                                             |\n",
    "|                          | `describe`, `summary`          | Computes basic statistics for numeric and string columns.                                                                                                                |\n",
    "|                          | `first`                       | Returns the first row.                                                                                                                                                  |\n",
    "|                          | `head`                        | Returns the first n rows.                                                                                                                                                 |\n",
    "|                          | `collect`                     | Returns an array that contains all rows in this DataFrame.                                                                                                               |\n",
    "|                          | `take`                        | Returns an array of the first n rows in the DataFrame.                                                                                                                    |\n",
    "| **DataFrameNaFunctions** | `drop`                        | Returns a new DataFrame omitting rows with any, all, or a specified number of null values, considering an optional subset of columns.                                    |\n",
    "|                          | `fill`                        | Replaces null values with the specified value for an optional subset of columns.                                                                                         |\n",
    "|                          | `replace`                     | Returns a new DataFrame replacing a value with another value, considering an optional subset of columns.                                                                |\n",
    "| **Built-in Functions**  |  **Math functions** | |\n",
    "|                          | `ceil`                         | Computes the ceiling of the given column.                                                                                                                                  |\n",
    "|                          | `log`                          | Computes the natural logarithm of the given value.                                                                                                                        |\n",
    "|                          | `round`                        | Returns the value of the column e rounded to 0 decimal places with HALF_UP round mode.                                                                                    |\n",
    "|                          | `sqrt`                         | Computes the square root of the specified float value.                                                                                                                     |\n",
    "|                          | **Collection functions**       |                                                                                                                                                                      |\n",
    "|                          | `array_contains`               | Returns null if the array is null, true if the array contains value, and false otherwise.                                                                             |\n",
    "|                          | `explode`                      | Creates a new row for each element in the given array or map column.                                                                                                      |\n",
    "|                          | `slice`                        | Returns an array containing all the elements in x from an index start (or from the end if start is negative) with the specified length.                              |\n",
    "|                         | **Date time functions**  | |\n",
    "|                          | `date_format`                  | Converts a date/timestamp/string to a value of a string in the format specified by the date format given by the second argument.                                           |\n",
    "|                          | `add_months`                   | Returns the date that is numMonths after startDate.                                                                                                                     |\n",
    "|                          | `dayofweek`                    | Extracts the day of the month as an integer from a given date/timestamp/string.                                                                                           |\n",
    "|                          | `from_unixtime`                | Converts the number of seconds from the unix epoch to a string representing the timestamp of that moment in the current system time zone in the yyyy-MM-dd HH:mm:ss format.|\n",
    "|                          | `minute`                       | Extracts the minutes as an integer from a given date/timestamp/string.                                                                                                   |\n",
    "|                          | `unix_timestamp`              | Converts a time string with a given pattern to a Unix timestamp (in seconds).                                                                                            |\n",
    "|                         | **String functions**  | |\n",
    "|                          | `translate`                  | Translate any character in the src by a character in replaceString                                                                                                     |\n",
    "|                          | `regexp_replace`             | Replace all substrings of the specified string value that match regexp with rep                                                                                        |\n",
    "|                          | `regexp_extract`             | Extract a specific group matched by a Java regex, from the specified string column                                                                                     |\n",
    "|                          | `ltrim`                      | Extract a specific group matched by a Java regex, from the specified string column                                                                                     |\n",
    "|                          | `lower`                     | Converts a string column to lowercase                                                                                     |\n",
    "|                          | `split`                     | Splits str around matches of the given pattern                                                                                     |\n",
    "| **Row Methods (Python)**    | `index`                       | Returns the first index of value.                                                                                                                                        |\n",
    "|                          | `count`                       | Returns the number of occurrences of value.                                                                                                                                |\n",
    "|                          | `asDict`                      | Returns a row as a dictionary.                                                                                                                                           |\n",
    "|                          | `row.key`                       | Access fields like attributes.                                                                                                                                            |\n",
    "|                          | `row[\"key\"]`                    | Access fields like dictionary values.                                                                                                                                   |\n",
    "|                          | `key in row`                   | Search through row keys.                                                                                                                                                |\n",
    "| **Grouped Data Object** |  `agg`    | Compute aggregates by specifying a series of aggregate columns|\n",
    "|                         |  `avg`    | Compute the mean value for each numeric columns for each group|\n",
    "|                         |  `count`  | Count the number of rows for each group|\n",
    "|                         |  `max`    | Compute the maximum value for each numeric column for each group|\n",
    "|                         |  `mean`   | Compute the average value for each numeric column for each group|\n",
    "|                         |  `min`    | Compute the minimum value for each numeric column for each group|\n",
    "|                         |  `pivot`  | Pivots a column of the current DataFrame and performs the specified aggregation|\n",
    "|                         |  `sum`    | Compute the sum for each numeric columns for each group|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1ada1e5-33e5-4fdb-b217-ea64590cc4df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F  # Importing common Spark functions with alias F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType, DateType  # Importing data types for schema definition\n",
    "from datetime import datetime # Importing datetime for date generation\n",
    "\n",
    "# 1. Create DataFrame with 1000 rows\n",
    "# ----------------------------------\n",
    "num_rows = 1000  # Defining the number of rows for the DataFrame\n",
    "\n",
    "# Defining the schema of the DataFrame using StructType and StructField\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),  # Integer column 'id'\n",
    "    StructField(\"category\", StringType(), True),  # String column 'category'\n",
    "    StructField(\"value\", IntegerType(), True),  # Integer column 'value'\n",
    "    StructField(\"items\", ArrayType(StringType()), True),  # Array of Strings column 'items'\n",
    "    StructField(\"date\", DateType(), True)  # Date column 'date'\n",
    "])\n",
    "\n",
    "# Creating sample data for the DataFrame\n",
    "data = [\n",
    "    (i, \"A\" if i % 3 == 0 else \"B\" if i % 2 == 0 else \"C\", i * 2, [f\"item_{i % 5}\", f\"item_{i % 7}\"], datetime.now().date())\n",
    "    for i in range(num_rows)\n",
    "]\n",
    "# Creating the DataFrame using the data and schema\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "\n",
    "# 2. SparkSession Methods\n",
    "# -----------------------\n",
    "# The SparkSession methods are used to create DataFrames, or to run SQL Queries on the DataFrames.\n",
    "\n",
    "# sql: creates a temporary view so that we can use the sql method to query the data.\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "# Using spark.sql to run a SQL query against the temporary view.\n",
    "sql_df = spark.sql(\"SELECT * FROM my_table WHERE value > 500\")\n",
    "print(\"\\nDataFrame after spark.sql():\")\n",
    "sql_df.show(5)  # Showing the first 5 rows of the resulting DataFrame\n",
    "\n",
    "\n",
    "# 3. DataFrame Transformation Methods\n",
    "# -------------------------------------\n",
    "# These are methods that modify the DataFrame and return a new transformed DataFrame without modifying the source DataFrame.\n",
    "# They perform lazy operations, which are performed when an action like show() is performed.\n",
    "\n",
    "# --- select() ---\n",
    "# selects columns from the DataFrame, and can create new ones based on expressions.\n",
    "df_selected = df.select(\"id\", \"category\", (F.col(\"value\") * 2).alias(\"double_value\"),\n",
    "                        F.col(\"items\").alias(\"my_items\"))\n",
    "print(\"\\nDataFrame after select():\")\n",
    "df_selected.show(5)\n",
    "\n",
    "# --- drop() ---\n",
    "# Drops the specified column from the DataFrame.\n",
    "df_dropped = df.drop(\"items\")\n",
    "print(\"\\nDataFrame after drop():\")\n",
    "df_dropped.show(5)\n",
    "\n",
    "# --- withColumnRenamed() ---\n",
    "# Renames the specified column with a new name\n",
    "df_renamed = df.withColumnRenamed(\"category\", \"group\")\n",
    "print(\"\\nDataFrame after withColumnRenamed():\")\n",
    "df_renamed.show(5)\n",
    "\n",
    "# --- withColumn() ---\n",
    "# adds a new column to the DataFrame, or replaces an existing column with a new value.\n",
    "df_with_column = df.withColumn(\"value_plus_10\", F.col(\"value\") + 10)\n",
    "print(\"\\nDataFrame after withColumn (adding a column):\")\n",
    "df_with_column.show(5)\n",
    "\n",
    "df_with_column_2 = df.withColumn(\"value\", F.lit(100))  # Replacing the value column with literal 100\n",
    "print(\"\\nDataFrame after withColumn (replacing a column):\")\n",
    "df_with_column_2.show(5)\n",
    "\n",
    "# --- filter(), where() ---\n",
    "# Filters rows of the DataFrame based on the given expression (the where() is the alias for filter())\n",
    "df_filtered = df.filter(F.col(\"value\") > 100)\n",
    "print(\"\\nDataFrame after filter()/where():\")\n",
    "df_filtered.show(5)\n",
    "\n",
    "# --- sort(), orderBy() ---\n",
    "# Sorts rows of the DataFrame based on the given column (orderBy() is the alias for sort())\n",
    "df_sorted = df.sort(F.col(\"value\"), ascending=False)\n",
    "print(\"\\nDataFrame after sort()/orderBy():\")\n",
    "df_sorted.show(5)\n",
    "\n",
    "\n",
    "# --- dropDuplicates(), distinct() ---\n",
    "# Removes the duplicated rows of the DataFrame (distinct is the alias for dropDuplicates())\n",
    "df_with_dupes = df.union(df.limit(5))  # Creating a DataFrame with duplicate rows\n",
    "df_no_dupes = df_with_dupes.dropDuplicates()\n",
    "print(\"\\nDataFrame after dropDuplicates()/distinct():\")\n",
    "df_no_dupes.show(5)\n",
    "\n",
    "\n",
    "# --- limit() ---\n",
    "# limits the DataFrame to the given number of rows\n",
    "df_limited = df.limit(10)\n",
    "print(\"\\nDataFrame after limit():\")\n",
    "df_limited.show(5)\n",
    "\n",
    "\n",
    "# --- groupBy() ---\n",
    "# Groups the DataFrame by the specified columns and can run aggregations.\n",
    "df_grouped = df.groupBy(\"category\").count()\n",
    "print(\"\\nDataFrame after groupBy():\")\n",
    "df_grouped.show(5)\n",
    "\n",
    "\n",
    "# 4. DataFrame Action Methods\n",
    "# ----------------------------\n",
    "# These methods trigger the computation and return results (non-lazy operations)\n",
    "\n",
    "# --- show() ---\n",
    "# Displays a given number of rows of the DataFrame in a tabular format\n",
    "print(\"\\nDataFrame show():\")\n",
    "df.show(3)  # Showing the first 3 rows of the DataFrame\n",
    "\n",
    "\n",
    "# --- count() ---\n",
    "# Returns the number of rows in the DataFrame\n",
    "print(f\"\\nDataFrame count(): {df.count()}\")\n",
    "\n",
    "# --- describe(), summary() ---\n",
    "# Computes basic descriptive statistics for each column of the DataFrame\n",
    "print(\"\\nDataFrame describe():\")\n",
    "df.describe().show() # computes statistics for numeric columns.\n",
    "print(\"\\nDataFrame summary():\")\n",
    "df.summary().show() # computes statistics for numeric and string columns.\n",
    "\n",
    "\n",
    "# --- first() ---\n",
    "# Returns the first row of the DataFrame\n",
    "print(f\"\\nDataFrame first(): {df.first()}\")\n",
    "\n",
    "# --- head() ---\n",
    "# Returns the first 'n' rows of the DataFrame\n",
    "print(\"\\nDataFrame head():\")\n",
    "print(df.head(3)) # returns as python list of Row objects\n",
    "\n",
    "# --- collect() ---\n",
    "# Collects all the rows of the DataFrame into a python list\n",
    "print(f\"\\nDataFrame collect(): (first 3 rows) {df.collect()[:3]}\")\n",
    "\n",
    "# --- take() ---\n",
    "# returns the first n rows of the DataFrame as a python list.\n",
    "print(f\"\\nDataFrame take(): (first 3 rows) {df.take(3)}\")\n",
    "\n",
    "\n",
    "# 5. DataFrameNaFunctions\n",
    "# -----------------------\n",
    "# These methods allow to handle null values on the DataFrame.\n",
    "\n",
    "# Creating a DataFrame with null values for demonstration purposes\n",
    "df_with_nulls = df.withColumn(\"value\", F.when(F.col(\"id\") % 10 == 0, F.lit(None)).otherwise(F.col(\"value\")))\n",
    "\n",
    "# --- drop() ---\n",
    "# Drops rows that have null values, can be configured for specific columns.\n",
    "df_na_dropped = df_with_nulls.na.drop()\n",
    "print(\"\\nDataFrame after na.drop():\")\n",
    "df_na_dropped.show(5)\n",
    "\n",
    "# --- fill() ---\n",
    "# Replaces all the null values of the given column with the value provided, can be configured for specific columns.\n",
    "df_na_filled = df_with_nulls.na.fill({\"value\": -1}) #replaces null values in value column with -1\n",
    "print(\"\\nDataFrame after na.fill():\")\n",
    "df_na_filled.show(5)\n",
    "\n",
    "# --- replace() ---\n",
    "# Replaces a value with another specified value in the given column (can use other types as well).\n",
    "df_na_replaced = df_with_nulls.na.replace(100, 1000, subset=[\"value\"])\n",
    "print(\"\\nDataFrame after na.replace():\")\n",
    "df_na_replaced.show(5)\n",
    "\n",
    "\n",
    "# 6. Built-in Functions\n",
    "# -----------------------\n",
    "# These are functions that can be used in expressions for column manipulation.\n",
    "\n",
    "# Math Functions:\n",
    "# Methods that perform math related functions.\n",
    "df_math_functions = df.withColumn(\"ceil_value\", F.ceil(\"value\")) \\\n",
    "    .withColumn(\"log_value\", F.log(\"value\")) \\\n",
    "    .withColumn(\"round_value\", F.round(\"value\", 0)) \\\n",
    "    .withColumn(\"sqrt_value\", F.sqrt(\"value\"))\n",
    "print(\"\\nDataFrame with Math Functions:\")\n",
    "df_math_functions.select(\"value\", \"ceil_value\", \"log_value\", \"round_value\", \"sqrt_value\").show(5)\n",
    "\n",
    "# Collection Functions:\n",
    "# These are functions that can be used for working with arrays or maps.\n",
    "df_collection_functions = df.withColumn(\"array_contains\", F.array_contains(\"items\", \"item_1\")) \\\n",
    "                            .withColumn(\"exploded_items\", F.explode(\"items\"))\n",
    "print(\"\\nDataFrame with Collection Functions:\")\n",
    "df_collection_functions.select(\"items\", \"array_contains\", \"exploded_items\").show(5)\n",
    "\n",
    "\n",
    "df_collection_functions_2 = df.withColumn(\"sliced_items\", F.slice(\"items\", 1, 1))\n",
    "print(\"\\nDataFrame with Collection Functions and slice\")\n",
    "df_collection_functions_2.select(\"items\", \"sliced_items\").show(5)\n",
    "\n",
    "\n",
    "# Date time Functions:\n",
    "# These functions manipulate date related columns.\n",
    "df_date_functions = df.withColumn(\"formatted_date\", F.date_format(\"date\", \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"add_months_date\", F.add_months(\"date\", 1)) \\\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"date\")) \\\n",
    "    .withColumn(\"minute\", F.minute(\"date\")) \\\n",
    "    .withColumn(\"unix_timestamp\", F.unix_timestamp(\"date\")) \\\n",
    "    .withColumn(\"from_unixtime\", F.from_unixtime(F.unix_timestamp(\"date\")))\n",
    "print(\"\\nDataFrame with Date Time Functions:\")\n",
    "df_date_functions.select(\"date\", \"formatted_date\", \"add_months_date\", \"day_of_week\", \"minute\", \"unix_timestamp\", \"from_unixtime\").show(5)\n",
    "\n",
    "\n",
    "# String functions\n",
    "# These functions manipulate String columns.\n",
    "df_string_functions = df.withColumn(\"translated_cat\", F.translate(\"category\", \"A\", \"Z\")) \\\n",
    "    .withColumn(\"replaced_category\", F.regexp_replace(\"category\", \"A\", \"AAA\")) \\\n",
    "    .withColumn(\"extracted_category\", F.regexp_extract(\"category\", \"(A|B)\", 1)) \\\n",
    "    .withColumn(\"trimmed_category\", F.ltrim(\"   \" + F.col(\"category\") + \"  \")) \\\n",
    "    .withColumn(\"lowered_category\", F.lower(\"category\")) \\\n",
    "    .withColumn(\"split_category\", F.split(\"category\", \"\"))\n",
    "print(\"\\nDataFrame with String Functions:\")\n",
    "df_string_functions.select(\"category\", \"translated_cat\", \"replaced_category\", \"extracted_category\",\n",
    "                           \"trimmed_category\", \"lowered_category\", \"split_category\").show(5)\n",
    "\n",
    "\n",
    "# 7. Row Methods (Example with first() row object)\n",
    "# -----------------------------------------------\n",
    "# methods that can be called when using Row objects.\n",
    "first_row = df.first()  # Getting the first row of the DataFrame as Row object\n",
    "print(f\"\\nFirst row as is: {first_row}\")\n",
    "\n",
    "print(f\"\\nFirst row index of value 'id': {first_row.index(0)}\")\n",
    "print(f\"\\nFirst row count of value 'A': {first_row.count('A')}\")\n",
    "print(f\"\\nFirst row as a dictionary: {first_row.asDict()}\")\n",
    "print(f\"\\nFirst row attribute access: {first_row.id}\")\n",
    "print(f\"\\nFirst row dictionary access: {first_row['category']}\")\n",
    "print(f\"\\n'id' in first row: {'id' in first_row}\")\n",
    "\n",
    "\n",
    "# 8. Grouped Data Object Methods\n",
    "# ------------------------------\n",
    "# these methods are called on the object returned after calling the groupby() method on the DataFrame\n",
    "grouped_df = df.groupBy(\"category\")\n",
    "\n",
    "# Using agg method to call multiple aggregations.\n",
    "df_aggregated = grouped_df.agg(\n",
    "    F.avg(\"value\").alias(\"avg_value\"),\n",
    "    F.count(\"*\").alias(\"count\"),\n",
    "    F.max(\"value\").alias(\"max_value\"),\n",
    "    F.min(\"value\").alias(\"min_value\"),\n",
    "    F.sum(\"value\").alias(\"sum_value\")\n",
    ")\n",
    "print(\"\\nDataFrame after groupBy().agg():\")\n",
    "df_aggregated.show(5)\n",
    "\n",
    "\n",
    "# Using avg, count, max, min and sum individually to show their usage.\n",
    "df_aggregated_2 = grouped_df.avg(\"value\").withColumnRenamed(\"avg(value)\", \"avg_value\")\n",
    "df_aggregated_2 = df_aggregated_2.join(grouped_df.count().withColumnRenamed(\"count\", \"count_rows\"), \"category\")\n",
    "df_aggregated_2 = df_aggregated_2.join(grouped_df.max(\"value\").withColumnRenamed(\"max(value)\", \"max_value\"), \"category\")\n",
    "df_aggregated_2 = df_aggregated_2.join(grouped_df.min(\"value\").withColumnRenamed(\"min(value)\", \"min_value\"), \"category\")\n",
    "df_aggregated_2 = df_aggregated_2.join(grouped_df.sum(\"value\").withColumnRenamed(\"sum(value)\", \"sum_value\"), \"category\")\n",
    "print(\"\\nDataFrame after groupBy().avg,count, max, min, sum():\")\n",
    "df_aggregated_2.show(5)\n",
    "\n",
    "\n",
    "#  pivot\n",
    "# pivots a table by selecting the values from the category column as new columns.\n",
    "df_pivoted = df.groupBy(\"id\").pivot(\"category\").sum(\"value\")\n",
    "print(\"\\nDataFrame after groupBy().pivot():\")\n",
    "df_pivoted.show(5)\n",
    "\n",
    "\n",
    "# 9. Column Operators & Methods\n",
    "# -------------------------------\n",
    "# methods and operators to manipulate columns.\n",
    "\n",
    "df_column_ops = df.withColumn(\"combined_value\", F.col(\"value\") + 10) \\\n",
    "    .withColumn(\"is_even\", (F.col(\"value\") % 2 == 0)) \\\n",
    "    .withColumn(\"double_value_cast\", F.col(\"value\").cast(\"double\")) \\\n",
    "    .withColumn(\"is_value_null\", F.col(\"value\").isNull())\n",
    "print(\"\\nDataFrame with Column Operations:\")\n",
    "df_column_ops.select(\"value\", \"combined_value\", \"is_even\", \"double_value_cast\", \"is_value_null\").show(5)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark Transforms and Actions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
