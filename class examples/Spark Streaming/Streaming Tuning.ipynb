{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea72c5b9-a102-4750-a6a1-9fdf5ab62998",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Includes"
    }
   },
   "outputs": [],
   "source": [
    "# Databricks / PySpark Diagnostic Notebook for Streaming Tuning\n",
    "from pyspark.sql.streaming import StreamingQueryListener\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "import time\n",
    "import threading\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58e1284d-4204-478d-88d5-d0fa4d5e9021",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Utility Functions"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Show last progress snapshot for a given query\n",
    "def show_query_progress(query):\n",
    "    progress = query.lastProgress\n",
    "    print(\"Last Progress Snapshot:\")\n",
    "    print(json.dumps(progress, indent=2))\n",
    "\n",
    "# 2. Display recommended tuning hints based on batch duration\n",
    "def diagnose_performance(query, interval=60):\n",
    "    progress = query.lastProgress\n",
    "    if not progress:\n",
    "        print(\"No progress yet. Query may not have started.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        batch_time = int(progress['durationMs']['triggerExecution']) / 1000\n",
    "        trigger_interval = interval  # Change based on your actual trigger\n",
    "\n",
    "        print(f\"\\nTrigger Interval: {trigger_interval} seconds\")\n",
    "        print(f\"Batch Duration: {batch_time:.2f} seconds\")\n",
    "\n",
    "        if batch_time > trigger_interval:\n",
    "            print(\"\\n[DIAGNOSIS] Batch is slower than trigger interval.\")\n",
    "            print(\"- Tune transformations to reduce load.\")\n",
    "            print(\"- Reduce input rate (e.g., maxOffsetsPerTrigger).\")\n",
    "            print(\"- Consider scaling up cluster cores or executors.\")\n",
    "        elif batch_time < trigger_interval * 0.5:\n",
    "            print(\"\\n[DIAGNOSIS] Batch is much faster than trigger.\")\n",
    "            print(\"- You may be underutilizing resources.\")\n",
    "            print(\"- Consider reducing trigger interval for lower latency.\")\n",
    "        else:\n",
    "            print(\"\\n[DIAGNOSIS] Trigger and batch are balanced.\")\n",
    "            print(\"- Good balance. Monitor input growth and state size.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Could not extract performance info:\", str(e))\n",
    "\n",
    "# 3. Optional: Attach a custom listener for logging\n",
    "class QueryLogger(StreamingQueryListener):\n",
    "    def onQueryProgress(self, event):\n",
    "        progress = event.progress\n",
    "        print(f\"[Batch {progress['batchId']}] {progress['timestamp']} | Rows: {progress['numInputRows']} | Duration: {progress['durationMs']['triggerExecution']}ms\")\n",
    "\n",
    "# 4. List all active streaming queries\n",
    "def list_active_queries():\n",
    "    queries = spark.streams.active\n",
    "    if not queries:\n",
    "        print(\"No active streaming queries.\")\n",
    "    else:\n",
    "        for i, q in enumerate(queries, 1):\n",
    "            print(f\"[{i}] ID: {q.id}, Name: {q.name}, Status: {q.status['message']}\")\n",
    "\n",
    "# 5. Stop all active queries (use with caution)\n",
    "def stop_all_queries():\n",
    "    for q in spark.streams.active:\n",
    "        print(f\"Stopping query ID: {q.id}, Name: {q.name}\")\n",
    "        q.stop()\n",
    "\n",
    "# 6. Simulate staged Delta writes for testing streaming ingestion\n",
    "def simulate_delta_commits(base_path=\"/tmp/delta/stream_source\", num_batches=50, rows_per_batch=1000, interval_sec=10):\n",
    "    for i in range(num_batches):\n",
    "        df = spark.range(i * rows_per_batch, (i + 1) * rows_per_batch).withColumn(\"ts\", current_timestamp())\n",
    "        df.write.format(\"delta\").mode(\"append\").save(base_path)\n",
    "        print(f\"Committed {rows_per_batch} rows to Delta at batch {i}\")\n",
    "        time.sleep(interval_sec)\n",
    "\n",
    "# 6b. Run simulate_delta_commits in the background thread\n",
    "def start_background_commits(base_path=\"/tmp/delta/stream_source\", num_batches=50, rows_per_batch=1000, interval_sec=20):\n",
    "    thread = threading.Thread(\n",
    "        target=simulate_delta_commits,\n",
    "        args=(base_path, num_batches, rows_per_batch, interval_sec),\n",
    "        daemon=True\n",
    "    )\n",
    "    thread.start()\n",
    "    print(\"Simulated Delta commit thread started.\")\n",
    "    return thread\n",
    "\n",
    "# 7. Attach a streaming query to the simulated Delta source\n",
    "def attach_read_stream(base_path=\"/tmp/delta/stream_source\", proc_time=\"5 seconds\"):\n",
    "    df = spark.readStream.format(\"delta\").load(base_path)\n",
    "    query = (\n",
    "        df.writeStream\n",
    "          .format(\"console\")\n",
    "          .outputMode(\"append\")\n",
    "          .option(\"checkpointLocation\", \"/tmp/delta/checkpoints/stream_test\")\n",
    "          .trigger(processingTime=proc_time)\n",
    "          .start()\n",
    "    )\n",
    "    print(\"Streaming query started.\")\n",
    "    return df, query\n",
    "\n",
    "# 8. Check partition count relative to cluster cores for batch DataFrames only\n",
    "def check_partitions_and_cores_static(df):\n",
    "    if df.isStreaming:\n",
    "        print(\"⚠️ Cannot check partitions on a streaming DataFrame. Try this after writing to Delta or converting to batch.\")\n",
    "        return\n",
    "    num_partitions = df.rdd.getNumPartitions()\n",
    "    total_cores = spark.sparkContext.defaultParallelism\n",
    "    print(f\"DataFrame partitions: {num_partitions}\")\n",
    "    print(f\"Available cluster cores: {total_cores}\")\n",
    "    if num_partitions < total_cores:\n",
    "        print(\"⚠️ Under-partitioned: not all cores will be used.\")\n",
    "    elif num_partitions > total_cores * 4:\n",
    "        print(\"⚠️ Over-partitioned: could lead to overhead and small files.\")\n",
    "    else:\n",
    "        print(\"✅ Partitioning is well balanced with available cores.\")\n",
    "\n",
    "# 9. Visualize task durations across batches for a query\n",
    "\n",
    "def plot_task_durations(query):\n",
    "    from IPython.display import display\n",
    "    durations = []\n",
    "    timestamps = []\n",
    "\n",
    "    progress_history = query.recentProgress\n",
    "    if not progress_history:\n",
    "        print(\"No progress history available.\")\n",
    "        return\n",
    "\n",
    "    for p in progress_history:\n",
    "        try:\n",
    "            durations.append(int(p['durationMs']['triggerExecution']) / 1000)\n",
    "            timestamps.append(p['timestamp'])\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if durations:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(timestamps, durations, marker='o', color='dodgerblue')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel(\"Trigger Duration (sec)\")\n",
    "        plt.title(\"Micro-Batch Execution Time over Time\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No valid durations to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c681b1ee-63a2-40c5-a50a-fe8d6abf4b8c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup a read/write stream"
    }
   },
   "outputs": [],
   "source": [
    "df, query = attach_read_stream(\"/tmp/delta/stream_source\", \"10 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a78729c4-1847-4b56-b4b6-5c26374bbd1a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Start Writing Data into the Stream"
    }
   },
   "outputs": [],
   "source": [
    "start_background_commits(\"/tmp/delta/stream_source\", 50, 2000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4322904-213b-4831-81bc-4f5c30223c38",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Look at the partitions of the read stream vs. cluster cores"
    }
   },
   "outputs": [],
   "source": [
    "check_partitions_and_cores_static(spark.read.format(\"delta\").load(\"/tmp/delta/stream_source\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e136e65a-c9cd-45a0-b228-e5b1d875fd55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_task_durations(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a41ae7b-c2a5-4633-bd37-c17e7c9ab894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "diagnose_performance(query,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1b88888-bff7-4a8c-8322-592bb42798f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Streaming Tuning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
