{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d92e1ae-00bd-4253-a7aa-38dc6ce4c242",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Functions"
    }
   },
   "outputs": [],
   "source": [
    "def get_streaming_stats():\n",
    "    \"\"\"\n",
    "    Retrieves streaming statistics such as elapsed time, input row count, and processing time.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe containing streaming statistics for active queries.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    start_time = None  # Track when the job started\n",
    "\n",
    "    for q in spark.streams.active:\n",
    "        progress = q.recentProgress\n",
    "        if progress:\n",
    "            for p in progress:\n",
    "                timestamp = datetime.strptime(p[\"timestamp\"], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "                # Set the start time on the first iteration\n",
    "                if start_time is None:\n",
    "                    start_time = timestamp\n",
    "\n",
    "                elapsed_time = (timestamp - start_time).total_seconds()  # Convert to seconds\n",
    "\n",
    "                # Check if 'addBatch' exists in 'durationMs' before accessing it\n",
    "                processing_time = p[\"durationMs\"].get(\"addBatch\", None) if \"durationMs\" in p else None\n",
    "\n",
    "                data.append({\n",
    "                    \"query\": q.name,\n",
    "                    \"elapsed_time\": elapsed_time,  # Time in seconds since job start\n",
    "                    \"input_rows\": p.get(\"numInputRows\", 0),  # Default to 0 if missing\n",
    "                    \"processing_time\": processing_time,  # Could be None if not available\n",
    "                    \"memory_used\": p.get(\"aggregatedStateOperators\", [{}])[0].get(\"stateMemory\", 0) if p.get(\"aggregatedStateOperators\") else 0\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def clean_up_delta_tables():\n",
    "    \"\"\"\n",
    "    Removes existing Delta tables for a fresh run.\n",
    "    \"\"\"\n",
    "    dbutils.fs.rm(delta_table_path, True)\n",
    "    dbutils.fs.rm(count_table_path, True)\n",
    "    dbutils.fs.rm(analytics_table_path, True)\n",
    "    dbutils.fs.rm(word_count_path, True)\n",
    "\n",
    "# Function to wait for the Delta table to be ready\n",
    "def wait_for_delta_table(path, timeout=30, check_interval=2):\n",
    "    \"\"\"\n",
    "    Waits for a Delta table to be available before proceeding.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the Delta table.\n",
    "        timeout (int): Maximum wait time in seconds.\n",
    "        check_interval (int): Time interval to check for table availability.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the table is ready, False otherwise.\n",
    "    \"\"\"\n",
    "    elapsed_time = 0\n",
    "    while elapsed_time < timeout:\n",
    "        try:\n",
    "            if spark.read.format(\"delta\").load(path).count() > 0:\n",
    "                return True\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(check_interval)\n",
    "        elapsed_time += check_interval\n",
    "    return False\n",
    "\n",
    "# Explicitly define a more complex schema for the JSON files\n",
    "json_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"subject\", StringType(), True),\n",
    "    StructField(\"value\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True),  # Added for text analysis\n",
    "    StructField(\"metrics\", StructType([  # Nested structure\n",
    "        StructField(\"temperature\", DoubleType(), True),\n",
    "        StructField(\"pressure\", DoubleType(), True),\n",
    "        StructField(\"humidity\", DoubleType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "def start_json_stream():\n",
    "    \"\"\"\n",
    "    Starts a Spark Structured Streaming job to ingest JSON files and write to Delta Lake.\n",
    "    Now with more files per trigger for increased load.\n",
    "\n",
    "    Returns:\n",
    "        StreamingQuery: A reference to the running streaming query.\n",
    "    \"\"\"\n",
    "    # Add some CPU-intensive transformations\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"maxFilesPerTrigger\", MAX_FILES_PER_TRIGGER)  # Increased from 1\n",
    "        .schema(json_schema)  # Apply the explicit schema\n",
    "        .load(input_path)\n",
    "        .withColumn(\"timestamp\", current_timestamp())\n",
    "        # Add CPU-intensive operations\n",
    "        .withColumn(\"random_value\", rand() * 100)  # Generate random values\n",
    "        .withColumn(\"value_squared\", expr(\"cast(value as double) * cast(value as double)\"))\n",
    "        .withColumn(\"complex_calculation\", \n",
    "                   expr(\"CASE WHEN metrics.temperature > 0 THEN LOG(metrics.temperature) * SQRT(metrics.pressure) ELSE 0 END\"))\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", f\"{delta_table_path}/_checkpoint\")\n",
    "        .option(\"mergeSchema\", \"true\")  # Enable schema merging\n",
    "        .trigger(processingTime=\"2 seconds\")  # Faster trigger\n",
    "        .queryName(\"JSON_Ingestion_Stream\")\n",
    "        .start(delta_table_path)\n",
    "    )\n",
    "    \n",
    "def start_count_stream():\n",
    "    \"\"\"\n",
    "    Starts a streaming query to compute row counts and timestamps from the Delta table.\n",
    "    Now with windowed aggregations for more complexity.\n",
    "\n",
    "    Returns:\n",
    "        StreamingQuery: A reference to the running count streaming query.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .load(delta_table_path)\n",
    "        .withWatermark(\"timestamp\", \"10 seconds\")  # Add watermarking\n",
    "        # Add window-based aggregation for more CPU load\n",
    "        .groupBy(\n",
    "            window(\"timestamp\", \"5 seconds\", \"2 seconds\")\n",
    "        )\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"row_count\"),\n",
    "            min(\"timestamp\").alias(\"min_timestamp\"),\n",
    "            max(\"timestamp\").alias(\"max_timestamp\"),\n",
    "            expr(\"percentile_approx(random_value, 0.5)\").alias(\"median_value\"),  # More complex aggregation\n",
    "            expr(\"stddev(value_squared)\").alias(\"std_dev\"),  # Add statistical calculation\n",
    "            expr(\"avg(complex_calculation)\").alias(\"avg_complex\")\n",
    "        )\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")  # Changed to append for windowed aggregation\n",
    "        .option(\"checkpointLocation\", f\"{count_table_path}/_checkpoint\")\n",
    "        .queryName(\"Row_Count_Stream\")\n",
    "        .trigger(processingTime=\"3 seconds\")\n",
    "        .start(count_table_path)\n",
    "    )\n",
    "\n",
    "def start_word_count_stream():\n",
    "    \"\"\"\n",
    "    Adds a text analysis stream to process text fields and count words.\n",
    "    \n",
    "    Returns:\n",
    "        StreamingQuery: A reference to the running word count streaming query.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .load(delta_table_path)\n",
    "        .select(\"id\", \"text\", \"timestamp\")\n",
    "        .filter(col(\"text\").isNotNull())\n",
    "        # Explode the text into words for counting\n",
    "        .withColumn(\"word\", explode(split(col(\"text\"), \" \")))\n",
    "        .groupBy(\"word\")\n",
    "        .agg(count(\"*\").alias(\"count\"))\n",
    "        .orderBy(col(\"count\").desc())\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"complete\")\n",
    "        .option(\"checkpointLocation\", f\"{word_count_path}/_checkpoint\")\n",
    "        .queryName(\"Word_Count_Stream\")\n",
    "        .trigger(processingTime=\"4 seconds\")\n",
    "        .start(word_count_path)\n",
    "    )\n",
    "\n",
    "def start_analytics_stream():\n",
    "    \"\"\"\n",
    "    Adds a more complex analytics stream with joins and multiple aggregations.\n",
    "    \n",
    "    Returns:\n",
    "        StreamingQuery: A reference to the running analytics streaming query.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .load(delta_table_path)\n",
    "        .withWatermark(\"timestamp\", \"15 seconds\")\n",
    "        .groupBy(\n",
    "            col(\"category\"),\n",
    "            window(\"timestamp\", \"10 seconds\", \"5 seconds\")\n",
    "        )\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"record_count\"),\n",
    "            expr(\"avg(metrics.temperature)\").alias(\"avg_temp\"),\n",
    "            expr(\"max(metrics.pressure)\").alias(\"max_pressure\"),\n",
    "            expr(\"min(metrics.humidity)\").alias(\"min_humidity\"),\n",
    "            expr(\"stddev(value_squared)\").alias(\"value_stddev\"),\n",
    "            expr(\"sum(complex_calculation)\").alias(\"total_complex\")\n",
    "        )\n",
    "        .select(\n",
    "            col(\"category\"),\n",
    "            col(\"window.start\").alias(\"window_start\"),\n",
    "            col(\"window.end\").alias(\"window_end\"),\n",
    "            col(\"record_count\"),\n",
    "            col(\"avg_temp\"),\n",
    "            col(\"max_pressure\"),\n",
    "            col(\"min_humidity\"),\n",
    "            col(\"value_stddev\"),\n",
    "            col(\"total_complex\"),\n",
    "            expr(\"total_complex / record_count\").alias(\"complex_per_record\")\n",
    "        )\n",
    "        .writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", f\"{analytics_table_path}/_checkpoint\")\n",
    "        .queryName(\"Analytics_Stream\")\n",
    "        .trigger(processingTime=\"5 seconds\")\n",
    "        .start(analytics_table_path)\n",
    "    )\n",
    "\n",
    "def generate_json_files(dbfs_path, num_files=5):\n",
    "    \"\"\"\n",
    "    Generates more complex JSON files with additional fields for the enhanced test.\n",
    "\n",
    "    Args:\n",
    "        dbfs_path (str): The DBFS directory to write the JSON files to.\n",
    "        num_files (int): The number of JSON files to generate.\n",
    "    \"\"\"\n",
    "    categories = [\"sensor\", \"device\", \"system\", \"network\", \"application\"]\n",
    "    texts = [\n",
    "        \"This is a sample text with multiple words for analysis purposes\",\n",
    "        \"Spark streaming performance testing with complex data structures\",\n",
    "        \"Big data processing at scale requires efficient resource management\",\n",
    "        \"Streaming analytics can provide real-time insights into your data\",\n",
    "        \"Optimizing Spark jobs is essential for maintaining performance\"\n",
    "    ]\n",
    "\n",
    "    for i in tqdm(range(num_files)):\n",
    "        file_name = f\"data_{i}.json\"\n",
    "        full_dbfs_path = f\"{dbfs_path}/{file_name}\"\n",
    "        local_tmp_path = f\"/tmp/{file_name}\"  # Use a temporary local path\n",
    "\n",
    "        # Generate more complex data\n",
    "        data = {\n",
    "            \"id\": f\"ID-{i}\",\n",
    "            \"subject\": f\"Subject_{i % 10}\",\n",
    "            \"value\": str(i * 10),  # Convert to string for stringType\n",
    "            \"category\": categories[i % len(categories)],\n",
    "            \"text\": texts[i % len(texts)] + f\" iteration {i}\",\n",
    "            \"metrics\": {\n",
    "                \"temperature\": round(20 + (i % 15) + np.random.random(), 2),\n",
    "                \"pressure\": round(1000 + (i % 50) + np.random.random() * 10, 2),\n",
    "                \"humidity\": round(30 + (i % 70) + np.random.random() * 5, 2)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Write JSON to a local temp file\n",
    "        with open(local_tmp_path, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "        # Copy the file to DBFS\n",
    "        dbutils.fs.cp(f\"file://{local_tmp_path}\", full_dbfs_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utilities",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
