{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f6ddef3-b1a4-469d-b9c6-bfd258ec4621",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark Streaming Resource Pool Performance Demo\n",
    "\n",
    "This demo showcases the impact of Spark's fair scheduler pools on streaming performance under high load. The script runs multiple concurrent streaming jobs with varying resource demands to illustrate how pools can improve consistency and throughput in resource-constrained environments.\n",
    "\n",
    "Key elements:\n",
    "\n",
    "1. **Workload Simulation**: Processes complex JSON data with nested structures through multiple streaming pipelines, including data ingestion, windowed aggregations, text analysis, and statistical calculations.\n",
    "\n",
    "2. **Resource Contention**: Creates deliberate competition for cluster resources by running four concurrent streams with overlapping processing windows and CPU-intensive operations.\n",
    "\n",
    "3. **Comparison Methodology**: Toggles between using dedicated scheduler pools (`USE_POOLS=True`) and shared resources (`USE_POOLS=False`) to demonstrate performance differences.\n",
    "\n",
    "4. **Performance Metrics**: Tracks and visualizes processing time, input rows, and memory usage across streams to illustrate how pool allocation affects execution consistency.\n",
    "\n",
    "5. **Visualization**: Generates time-series plots and summary statistics to quantify the impact of resource pools on processing latency and throughput.\n",
    "\n",
    "This demo helps students understand when and why resource pools should be implemented in production Spark streaming applications, especially in multi-tenant environments or with complex workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0695b8a6-5ca6-4e56-859b-a60cbd3326d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./includes/includes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1be511cc-c870-4e04-aa80-0024d47c2df3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97836520-5e5e-426d-9683-4aa695f6f19d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Control Variables\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "USE_POOLS = True\n",
    "GEN_INPUT_FILES = False\n",
    "FILE_COUNT = 20000  # Increased file count\n",
    "CONCURRENT_STREAMS = 4  # Run multiple streams concurrently\n",
    "MAX_FILES_PER_TRIGGER = 10  # Process more files simultaneously\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfaab5d4-c228-4f0f-b488-6e5e6885be06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Demo Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7758d7c-8aef-410f-bd36-391254d54394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if GEN_INPUT_FILES:\n",
    "    # Clean up the previous runs\n",
    "    dbutils.fs.rm(input_path, True)\n",
    "    generate_json_files(input_path, FILE_COUNT)\n",
    "\n",
    "\n",
    "### Start Multiple Concurrent Streams\n",
    "\n",
    "clean_up_delta_tables()\n",
    "\n",
    "if USE_POOLS:\n",
    "    # Initialize Spark session with resource allocation for pools\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")  # Moderate parallelism\n",
    "    spark.conf.set(\"spark.default.parallelism\", \"8\")\n",
    "    spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")  # Disable adaptive query execution to make pool effects clearer\n",
    "    \n",
    "    # Set up fair scheduler pools\n",
    "    spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"default\")\n",
    "    json_stream = start_json_stream()\n",
    "    \n",
    "    # wait for the delta table to ensure it exists\n",
    "    wait_for_delta_table(delta_table_path)\n",
    "    \n",
    "    # Now start all streams in different pools concurrently\n",
    "    spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"count_pool\")\n",
    "    count_stream = start_count_stream()\n",
    "    # wait for the delta table to ensure it exists\n",
    "    wait_for_delta_table(count_table_path)\n",
    "\n",
    "    spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"word_pool\")\n",
    "    word_stream = start_word_count_stream()\n",
    "    # wait for the delta table to ensure it exists\n",
    "    wait_for_delta_table(word_count_path)\n",
    "\n",
    "    spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"analytics_pool\")\n",
    "    analytics_stream = start_analytics_stream()\n",
    "    # wait for the delta table to ensure it exists\n",
    "    wait_for_delta_table(analytics_table_path)   \n",
    "else:\n",
    "    # Without pools, all streams will compete for the same resources\n",
    "    json_stream = start_json_stream()\n",
    "    \n",
    "    # Start the delta table to ensure it exists\n",
    "    wait_for_delta_table(delta_table_path)\n",
    "    \n",
    "    # Start all streams concurrently without pool separation\n",
    "    count_stream = start_count_stream()\n",
    "    wait_for_delta_table(count_table_path)\n",
    "\n",
    "    word_stream = start_word_count_stream()\n",
    "    wait_for_delta_table(word_count_path)\n",
    "\n",
    "    analytics_stream = start_analytics_stream()\n",
    "    wait_for_delta_table(analytics_table_path)   \n",
    "\n",
    "# Start monitoring in a separate loop\n",
    "print(f\"Monitoring streams with USE_POOLS = {USE_POOLS}\")\n",
    "\n",
    "\"\"\"\n",
    "Monitor the streams until all of the data is processed\n",
    "\"\"\"\n",
    "row_count = 1\n",
    "i = 0\n",
    "while row_count != 0:\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # Get streaming stats\n",
    "    stats = get_streaming_stats()\n",
    "    \n",
    "    # Display current status\n",
    "    if not stats.empty:\n",
    "        # Group by query name and get the latest stats\n",
    "        latest_stats = stats.sort_values(\"elapsed_time\").groupby(\"query\").last().reset_index()\n",
    "        print(f\"\\nStatus at {datetime.now().strftime('%H:%M:%S')} (Elapsed: {i*10}s):\")\n",
    "        for _, row in latest_stats.iterrows():\n",
    "            print(f\"  {row['query']}: {row['input_rows']} rows, {row['processing_time']}ms processing time\")\n",
    "        row_count = latest_stats[\"input_rows\"].sum()\n",
    "        i += 1\n",
    "\n",
    "# Collect final streaming metrics\n",
    "df = get_streaming_stats()\n",
    "\n",
    "# Plot processing time and input rows over elapsed time by query\n",
    "if not df.empty:\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "    \n",
    "    # Plot processing time\n",
    "    for query in df[\"query\"].unique():\n",
    "        subset = df[df[\"query\"] == query]\n",
    "        ax1.plot(subset[\"elapsed_time\"], subset[\"processing_time\"], marker='o', linestyle='-', label=query)\n",
    "    \n",
    "    ax1.set_ylabel(\"Processing Time (ms)\")\n",
    "    ax1.set_title(f\"Spark Streaming Processing Time - USE_POOLS = {USE_POOLS}\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot input rows\n",
    "    for query in df[\"query\"].unique():\n",
    "        subset = df[df[\"query\"] == query]\n",
    "        ax2.plot(subset[\"elapsed_time\"], subset[\"input_rows\"], marker='o', linestyle='-', label=query)\n",
    "    \n",
    "    ax2.set_xlabel(\"Elapsed Time (seconds)\")\n",
    "    ax2.set_ylabel(\"Input Rows\")\n",
    "    ax2.set_title(f\"Spark Streaming Input Rows - USE_POOLS = {USE_POOLS}\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate statistics summary\n",
    "    print(\"\\nStreaming Performance Summary:\")\n",
    "    summary = df.groupby(\"query\").agg({\n",
    "        \"processing_time\": [\"mean\", \"max\", \"min\", \"std\"],\n",
    "        \"input_rows\": [\"sum\", \"mean\", \"max\"]\n",
    "    }).reset_index()\n",
    "    display(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "472b4468-5a54-4534-a5a1-2ac5ab06aa64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stop all the active streams\n",
    "for s in spark.streams.active:\n",
    "    s.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark Structured Streaming Performance Analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
