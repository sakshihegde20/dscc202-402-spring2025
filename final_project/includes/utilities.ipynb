{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb9e3148-916a-455f-8dd1-e0dde07cdc35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.session import SparkSession\n",
    "import time\n",
    "import pandas as pd\n",
    "from delta import *\n",
    "import os\n",
    "\n",
    "# Function to optimize Delta table if it exists\n",
    "def optimize_table(path):\n",
    "    if os.path.exists(path):\n",
    "        DeltaTable.forPath(spark, path).optimize().executeCompaction()\n",
    "        print(f\"Optimized Delta Table at {path}\")\n",
    "    else:\n",
    "        print(f\"Delta Table at {path} does not exist.\")\n",
    "\n",
    "# This routine requires the paths defined in the includes notebook\n",
    "# and it clears data from the previous run.\n",
    "def clear_previous_run() -> bool:\n",
    "    # delete previous run \n",
    "    dbutils.fs.rm(BRONZE_CHECKPOINT, True)\n",
    "    dbutils.fs.rm(BRONZE_DELTA, True)\n",
    "    dbutils.fs.rm(SILVER_CHECKPOINT, True)\n",
    "    dbutils.fs.rm(SILVER_DELTA, True)\n",
    "    dbutils.fs.rm(GOLD_CHECKPOINT, True)\n",
    "    dbutils.fs.rm(GOLD_DELTA, True)\n",
    "    return True\n",
    "\n",
    "def stop_all_streams() -> bool:\n",
    "    stopped = False\n",
    "    for stream in spark.streams.active:\n",
    "        stopped = True\n",
    "        stream.stop()\n",
    "    return stopped\n",
    "\n",
    "\n",
    "def stop_named_stream(spark: SparkSession, namedStream: str) -> bool:\n",
    "    stopped = False\n",
    "    for stream in spark.streams.active:\n",
    "        if stream.name == namedStream:\n",
    "            stopped = True \n",
    "            stream.stop()\n",
    "    return stopped\n",
    "\n",
    "def wait_stream_start(spark: SparkSession, namedStream: str) -> bool:\n",
    "    started = False\n",
    "    count = 0\n",
    "    if started == False and count <= 3:\n",
    "        for stream in spark.streams.active:\n",
    "            if stream.name == namedStream:\n",
    "                started = True\n",
    "        count += 1\n",
    "        time.sleep(10)\n",
    "    return started    \n",
    "\n",
    "# Function to wait for the Delta table to be ready\n",
    "def wait_for_delta_table(path, timeout=30, check_interval=2):\n",
    "    \"\"\"\n",
    "    Waits for a Delta table to be available before proceeding.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the Delta table.\n",
    "        timeout (int): Maximum wait time in seconds.\n",
    "        check_interval (int): Time interval to check for table availability.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the table is ready, False otherwise.\n",
    "    \"\"\"\n",
    "    elapsed_time = 0\n",
    "    while elapsed_time < timeout:\n",
    "        try:\n",
    "            if spark.read.format(\"delta\").load(path).count() > 0:\n",
    "                return True\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(check_interval)\n",
    "        elapsed_time += check_interval\n",
    "    return False\n",
    "\n",
    "# Function to retrieve streaming statistics\n",
    "def get_streaming_stats():\n",
    "    \"\"\"\n",
    "    Retrieves streaming statistics such as elapsed time, input row count, and processing time.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe containing streaming statistics for active queries.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    start_time = None  # Track when the job started\n",
    "\n",
    "    for q in spark.streams.active:\n",
    "        progress = q.recentProgress\n",
    "        if progress:\n",
    "            for p in progress:\n",
    "                timestamp = datetime.strptime(p[\"timestamp\"], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "                # Set the start time on the first iteration\n",
    "                if start_time is None:\n",
    "                    start_time = timestamp\n",
    "\n",
    "                elapsed_time = (timestamp - start_time).total_seconds()  # Convert to seconds\n",
    "\n",
    "                # Check if 'addBatch' exists in 'durationMs' before accessing it\n",
    "                processing_time = p[\"durationMs\"].get(\"addBatch\", None) if \"durationMs\" in p else None\n",
    "\n",
    "                data.append({\n",
    "                    \"query\": q.name,\n",
    "                    \"elapsed_time\": elapsed_time,  # Time in seconds since job start\n",
    "                    \"input_rows\": p.get(\"numInputRows\", 0),  # Default to 0 if missing\n",
    "                    \"processing_time\": processing_time,  # Could be None if not available\n",
    "                    \"memory_used\": p.get(\"aggregatedStateOperators\", [{}])[0].get(\"stateMemory\", 0) if p.get(\"aggregatedStateOperators\") else 0\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utilities",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
